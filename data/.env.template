# Data Ingestion System Environment Variables
# Copy this file to .env and fill in your actual values

# ===========================================
# DATA PROVIDER API KEYS
# ===========================================

# Alpha Vantage (Primary data source)
ALPHA_VANTAGE_API_KEY=Z0WR10WWKFEH25JO

# Yahoo Finance (Backup/Secondary)
# Note: yfinance doesn't require an API key but has rate limits

# Polygon.io (Alternative provider)
POLYGON_API_KEY=your_polygon_api_key_here

# IEX Cloud (Alternative provider)
IEX_CLOUD_API_KEY=your_iex_cloud_api_key_here

# Twelve Data (Alternative provider)
TWELVE_DATA_API_KEY=your_twelve_data_api_key_here

# Quandl (Economic data)
QUANDL_API_KEY=your_quandl_api_key_here

# ===========================================
# DATABASE CONFIGURATION
# ===========================================

# SQLite Database Path (for catalog)
CATALOG_DB_PATH=data/catalog/data_catalog.db

# PostgreSQL (if using external database)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=market_data
POSTGRES_USER=your_db_user
POSTGRES_PASSWORD=your_db_password

# ===========================================
# STORAGE CONFIGURATION
# ===========================================

# Base storage path for parquet files
STORAGE_BASE_PATH=data/processed

# Temporary storage for downloads
TEMP_STORAGE_PATH=data/temp

# Cache directory for yfinance
YFINANCE_CACHE_PATH=.cache/yfinance

# ===========================================
# LOGGING CONFIGURATION
# ===========================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log file path
LOG_FILE_PATH=data/logs/pipeline.log

# Log retention (days)
LOG_RETENTION_DAYS=30

# ===========================================
# PIPELINE CONFIGURATION
# ===========================================

# Download batch size (number of symbols to process simultaneously)
DOWNLOAD_BATCH_SIZE=5

# Retry attempts for failed downloads
RETRY_ATTEMPTS=3

# Rate limit delay between API calls (seconds)
RATE_LIMIT_DELAY=1

# Maximum daily return threshold for outlier detection
MAX_DAILY_RETURN_THRESHOLD=0.15

# Quality score threshold for data acceptance
MIN_QUALITY_THRESHOLD=0.70

# ===========================================
# NOTIFICATION SETTINGS
# ===========================================

# Email notifications for pipeline failures
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your_email@gmail.com
SMTP_PASSWORD=your_app_password
NOTIFICATION_EMAIL=alerts@yourdomain.com

# Slack notifications
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK

# ===========================================
# MONITORING AND ALERTING
# ===========================================

# Enable quality monitoring
ENABLE_QUALITY_MONITORING=true

# Quality alert threshold (below this percentage triggers alert)
QUALITY_ALERT_THRESHOLD=0.80

# Performance monitoring
ENABLE_PERFORMANCE_MONITORING=true

# Memory usage alert threshold (MB)
MEMORY_ALERT_THRESHOLD=1000

# ===========================================
# TIMEZONE SETTINGS
# ===========================================

# Default timezone for data processing
DEFAULT_TIMEZONE=UTC

# Market timezone mappings
NYSE_TIMEZONE=America/New_York
ASX_TIMEZONE=Australia/Sydney
LSE_TIMEZONE=Europe/London

# ===========================================
# DATA VALIDATION SETTINGS
# ===========================================

# Maximum missing data percentage allowed
MAX_MISSING_DATA_PCT=0.05

# Outlier detection method (iqr, zscore, isolation_forest)
OUTLIER_DETECTION_METHOD=iqr

# IQR multiplier for outlier detection
IQR_MULTIPLIER=3.0

# Interpolation limit for missing values
INTERPOLATION_LIMIT=2

# ===========================================
# STORAGE OPTIMIZATION
# ===========================================

# Parquet compression type (snappy, gzip, lz4)
PARQUET_COMPRESSION=snappy

# Enable parquet metadata
ENABLE_PARQUET_METADATA=true

# Partition strategy (year, month, none)
PARTITION_STRATEGY=year

# ===========================================
# DEVELOPMENT SETTINGS
# ===========================================

# Debug mode (enables verbose logging)
DEBUG_MODE=false

# Enable profiling
ENABLE_PROFILING=false

# Test mode (uses sample data)
TEST_MODE=false

# Mock API responses (for testing without API calls)
MOCK_API_RESPONSES=false

# ===========================================
# SECURITY SETTINGS
# ===========================================

# Data encryption key (for sensitive data)
DATA_ENCRYPTION_KEY=your_32_character_encryption_key

# Access token for API authentication
API_ACCESS_TOKEN=your_api_access_token

# ===========================================
# CLOUD STORAGE (Optional)
# ===========================================

# AWS S3 Configuration
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-market-data-bucket

# Google Cloud Storage
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account-key.json
GCS_BUCKET_NAME=your-gcs-bucket

# Azure Blob Storage
AZURE_STORAGE_ACCOUNT_NAME=your_storage_account
AZURE_STORAGE_ACCOUNT_KEY=your_storage_key
AZURE_CONTAINER_NAME=market-data

# ===========================================
# BACKUP SETTINGS
# ===========================================

# Enable automatic backups
ENABLE_BACKUPS=true

# Backup frequency (daily, weekly, monthly)
BACKUP_FREQUENCY=weekly

# Backup retention period (days)
BACKUP_RETENTION_DAYS=90

# Backup destination
BACKUP_DESTINATION=data/backups

# ===========================================
# PERFORMANCE TUNING
# ===========================================

# Number of worker threads for parallel processing
WORKER_THREADS=4

# Memory limit for large dataset processing (MB)
MEMORY_LIMIT_MB=2000

# Chunk size for large dataset processing
CHUNK_SIZE=10000

# Connection timeout for API requests (seconds)
CONNECTION_TIMEOUT=30

# Read timeout for API requests (seconds)
READ_TIMEOUT=60
